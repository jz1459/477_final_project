{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "# IMPORTS\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter\n",
        "from torch.autograd import Variable\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "#CORPUS PATHS\n",
        "\n",
        "# Path to training corpus\n",
        "TRAIN_CORPUS = 'ours/train.txt'\n",
        "\n",
        "# Path to development corpus\n",
        "TEST_CORPUS = 'ours/dev.txt'\n",
        "\n",
        "# Path to saving the weights\n",
        "SAVE_CORPUS = 'ours/large_model.pt'\n",
        "#%%\n",
        "#SET DEVICE\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device(\"mps\")\n",
        "else:\n",
        "    if not torch.backends.mps.is_built():\n",
        "        print(\"MPS not available because the current PyTorch install was not \"\n",
        "              \"built with MPS enabled.\")\n",
        "    else:\n",
        "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
        "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"Warning: You are using CPU. For better performance, use GPU.\")\n",
        "print(\"Pytorch version is: \", torch.__version__)\n",
        "print(\"You are using: \", DEVICE)\n",
        "#%%\n",
        "# HYPER PARAMETERS\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "#NUM_EPOCHS\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "#Encoding/Decoding Layers\n",
        "NUM_LAYERS = 9\n",
        "\n",
        "#Num of Attention Heads\n",
        "NUM_HEADS = 8\n",
        "\n",
        "#Model Dimensions\n",
        "MODEL_DIMENSIONS = 512\n",
        "\n",
        "#Feed-Forward Dimensions\n",
        "FEED_FORWARD_DIMENSIONS = 2048\n",
        "\n",
        "#Dropout Rate\n",
        "DROPOUT = 0.1\n",
        "\n",
        "#Maximum Sentence Length\n",
        "MAX_LENGTH = 60\n",
        "\n",
        "#unknown ID\n",
        "UNK = 0\n",
        "\n",
        "#padding ID\n",
        "PAD = 1\n",
        "#%%\n",
        "def pad_sequence(sequences, padding=0):\n",
        "    lengths = []\n",
        "    for seq in sequences:\n",
        "        lengths.append(len(seq))\n",
        "    max_length = max(lengths)\n",
        "    result = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            padded_element = list(seq) + [padding] * (max_length - len(seq))\n",
        "        else:\n",
        "            padded_element = seq\n",
        "        result.append(padded_element)\n",
        "\n",
        "    return np.array(result)\n",
        "#%%\n",
        "\n",
        "class PreProcessing:\n",
        "    def __init__(self, train_corpus, test_corpus):\n",
        "        self.train_sequences_en, self.train_sequences_zh = self.generate_sequences_from_corpus(train_corpus)\n",
        "        self.test_sequences_en, self.test_sequences_zh = self.generate_sequences_from_corpus(test_corpus)\n",
        "        self.dict_en, self.dict_size_en, self.index_dict_en = self.generate_dictionary(self.train_sequences_en)\n",
        "        self.dict_zh, self.dict_size_zh, self.index_dict_zh = self.generate_dictionary(self.train_sequences_zh)\n",
        "        self.train_sequences_en, self.train_sequences_zh = self.map_word(self.train_sequences_en, self.train_sequences_zh, self.dict_en, self.dict_zh)\n",
        "        self.test_sequences_en, self.test_sequences_zh = self.map_word(self.test_sequences_en, self.test_sequences_zh, self.dict_en, self.dict_zh)\n",
        "        self.train_data = self.split_batch(self.train_sequences_en, self.train_sequences_zh, BATCH_SIZE)\n",
        "        self.test_data = self.split_batch(self.test_sequences_en, self.test_sequences_zh, BATCH_SIZE)\n",
        "\n",
        "    def generate_sequences_from_corpus(self, corpus):\n",
        "        en, zh = [], []\n",
        "        with open(corpus, 'r', encoding='utf-8') as file:\n",
        "            for pair in file:\n",
        "                pair = pair.strip().split('\\t')\n",
        "                en_sentence = [\"BOS\"]\n",
        "                zh_sentence = [\"BOS\"]\n",
        "                en_sentence.extend(word_tokenize(pair[0].lower()))\n",
        "                en_sentence.append(\"EOS\")\n",
        "                for word in pair[1]:\n",
        "                    zh_sentence.extend(word_tokenize(word))\n",
        "                zh_sentence.append(\"EOS\")\n",
        "                en.append(en_sentence)\n",
        "                zh.append(zh_sentence)\n",
        "        return en, zh\n",
        "\n",
        "    def generate_dictionary(self, sequences, max_length=50000):\n",
        "        word_count = Counter()\n",
        "        for seq in sequences:\n",
        "            for word in seq:\n",
        "                word_count[word] += 1\n",
        "        ls = word_count.most_common(max_length)\n",
        "        total_words = len(ls) + 2\n",
        "        word_dict = {}\n",
        "        index_dict = {}\n",
        "        for index, (word, _) in enumerate(ls, start=2):\n",
        "            word_dict[word] = index\n",
        "            index_dict[index] = word\n",
        "        word_dict['UNK'] = UNK\n",
        "        word_dict['PAD'] = PAD\n",
        "        index_dict[UNK] = 'UNK'\n",
        "        index_dict[PAD] = 'PAD'\n",
        "        return word_dict, total_words, index_dict\n",
        "\n",
        "    def map_word(self, sequences_en, sequences_zh, dict_en, dict_zh, sort=True):\n",
        "        tokens_en, tokens_zh = [], []\n",
        "        for en_sentence, zh_sentence in zip(sequences_en, sequences_zh):\n",
        "            en_ids = [dict_en.get(word, UNK) for word in en_sentence]\n",
        "            zh_ids = [dict_zh.get(word, UNK) for word in zh_sentence]\n",
        "            tokens_en.append(en_ids)\n",
        "            tokens_zh.append(zh_ids)\n",
        "        if sort:\n",
        "            sorted_indices = sorted(range(len(tokens_en)), key=lambda i: len(tokens_en[i]))\n",
        "            tokens_en = [tokens_en[i] for i in sorted_indices]\n",
        "            tokens_zh = [tokens_zh[i] for i in sorted_indices]\n",
        "        return tokens_en, tokens_zh\n",
        "\n",
        "\n",
        "    def split_batch(self, sequences_en, sequences_zh, batch_size, shuffle=True):\n",
        "        idx_list = list(range(0, len(sequences_en), batch_size))\n",
        "        if shuffle:\n",
        "            np.random.shuffle(idx_list)\n",
        "        batches = []\n",
        "        for idx in idx_list:\n",
        "            batch_indices = range(idx, min(idx + batch_size, len(sequences_en)))\n",
        "            batch_en = [sequences_en[i] for i in batch_indices]\n",
        "            batch_zh = [sequences_zh[i] for i in batch_indices]\n",
        "            batch_zh = pad_sequence(batch_zh)\n",
        "            batch_en = pad_sequence(batch_en)\n",
        "            batches.append(Batch(batch_en, batch_zh))\n",
        "        return batches\n",
        "#%%\n",
        "def compute_attention(query, key, value, mask=None, dropout_layer=None):\n",
        "    dimension_key = query.size(-1)\n",
        "    raw_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dimension_key)\n",
        "\n",
        "    if mask is not None:\n",
        "        raw_scores.masked_fill_(mask == 0, float('-inf'))\n",
        "\n",
        "    normalized_scores = F.softmax(raw_scores, dim=-1)\n",
        "\n",
        "    if dropout_layer is not None:\n",
        "        normalized_scores = dropout_layer(normalized_scores)\n",
        "\n",
        "    weighted_values = torch.matmul(normalized_scores, value)\n",
        "\n",
        "    return weighted_values, normalized_scores\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "#%%\n",
        "class CreateEmbeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(CreateEmbeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
        "        self.scale = math.sqrt(self.d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded_x = self.embedding(x)\n",
        "        scaled_embeddings = embedded_x * self.scale\n",
        "        return scaled_embeddings\n",
        "\n",
        "class EncodePositions(nn.Module):\n",
        "    def __init__(self, model_dim, dropout_rate, max_length=5000):\n",
        "        super(EncodePositions, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "        self.max_length = max_length\n",
        "        self.dropout_layer = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        positional_matrix = torch.zeros(self.max_length, self.model_dim, device=DEVICE)\n",
        "        positions = torch.arange(0, self.max_length, device=DEVICE).unsqueeze(1)\n",
        "        frequency_divisor = torch.exp(torch.arange(0, self.model_dim, 2, device=DEVICE) * -(math.log(10000.0) / self.model_dim))\n",
        "\n",
        "        positional_matrix[:, 0::2] = torch.sin(positions * frequency_divisor)\n",
        "        positional_matrix[:, 1::2] = torch.cos(positions * frequency_divisor)\n",
        "\n",
        "        self.register_buffer('positional_matrix', positional_matrix.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_matrix[:, :x.size(1)]\n",
        "        return self.dropout_layer(x)\n",
        "\n",
        "\n",
        "class MHA(nn.Module):\n",
        "    def __init__(self, head, model_dimensions, dropout=0.1):\n",
        "        super(MHA, self).__init__()\n",
        "\n",
        "        self.head_dimensions = model_dimensions // head\n",
        "        self.head = head\n",
        "        self.layers = clones(nn.Linear(model_dimensions, model_dimensions), 4)\n",
        "        self.attention = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        num_batches = q.size(0)\n",
        "        q, k, v = [layer(tensor).view(num_batches, -1, self.head, self.head_dimensions).transpose(1, 2)\n",
        "                   for layer, tensor in zip(self.layers, (q, k, v))]\n",
        "\n",
        "        attention_output, self.attention = compute_attention(q, k, v, mask=mask, dropout_layer=self.dropout)\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(num_batches, -1, self.head * self.head_dimensions)\n",
        "        return self.layers[-1](attention_output)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    def __init__(self, source, outputs=None, pad=0):\n",
        "        source = torch.from_numpy(source).to(DEVICE).long()\n",
        "        outputs = torch.from_numpy(outputs).to(DEVICE).long()\n",
        "\n",
        "        self.source = source\n",
        "        self.source_mask = (source != pad).unsqueeze(-2)\n",
        "\n",
        "        if outputs is not None:\n",
        "            self.outputs = outputs[:, :-1]\n",
        "            self.output_targets = outputs[:, 1:]\n",
        "            self.output_masks = self.generate_mask(self.outputs, pad)\n",
        "            self.token_count = (self.output_targets != pad).data.sum()\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_mask(target, padding):\n",
        "        result = (target != padding).unsqueeze(-2)\n",
        "        result = result & Variable(subsequent_mask(target.size(-1)).type_as(result.data))\n",
        "        return result\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, num_features, epsiolon=1e-6):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "\n",
        "        self.scale = nn.Parameter(torch.ones(num_features))\n",
        "        self.shift = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "        self.epsilon = epsiolon\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg = x.mean(-1, keepdim=True)\n",
        "        variance = x.var(-1, keepdim=True)\n",
        "        normalized = (x - avg) / torch.sqrt(variance + self.epsilon)\n",
        "        return self.scale * normalized + self.shift\n",
        "\n",
        "class ConnectionLayer(nn.Module):\n",
        "    def __init__(self, dimension, drop_rate):\n",
        "        super(ConnectionLayer, self).__init__()\n",
        "        self.normalization = LayerNormalization(dimension)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x, function):\n",
        "        return x + self.dropout(function(self.normalization(x)))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, model_dimensions, feed_forward_dimensions, drop_rate=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(model_dimensions, feed_forward_dimensions)\n",
        "        self.linear2 = nn.Linear(feed_forward_dimensions, model_dimensions)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "#%%\n",
        "# Transformer Architecture\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_embeddings, target_embeddings, generator):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.source_embeddings = source_embeddings\n",
        "        self.target_embeddings = target_embeddings\n",
        "        self.generator = generator\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def encode(self, source, source_mask):\n",
        "        return self.encoder(self.source_embeddings(source), source_mask)\n",
        "\n",
        "    def decode(self, memory, source_mask, target, target_mask):\n",
        "        return self.decoder(self.target_embeddings(target), memory, source_mask, target_mask)\n",
        "\n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        return self.decode(self.encode(source, source_mask), source_mask, target, target_mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.normalization = LayerNormalization(layer.size)\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, source_mask, target_mask)\n",
        "        return self.normalization(x)\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attention, source_attention, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.source_attention = source_attention\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(ConnectionLayer(size, dropout), 3)\n",
        "        self.size = size\n",
        "        self.self_attention = self_attention\n",
        "\n",
        "    def forward(self, x, memory, source_mask, target_mask):\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, target_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.source_attention(x, m, m, source_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.normalization = LayerNormalization(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.normalization(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attention, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.sublayer = clones(ConnectionLayer(size, dropout), 2)\n",
        "        self.size = size\n",
        "        self.self_attention = self_attention\n",
        "        self.feed_forward = feed_forward\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attention(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "#%%\n",
        "# Building Model\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    def __init__(self, size, padding_index, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
        "        self.padding_index = padding_index\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        dist = x.data.clone()\n",
        "        dist.fill_(self.smoothing / (self.size - 2))\n",
        "        dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        dist[:, self.padding_index] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_index, as_tuple=False)\n",
        "        if mask.dim() > 0:\n",
        "            dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        return self.criterion(x, Variable(dist, requires_grad=False))\n",
        "\n",
        "class LossComputation:\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.data.item() * norm.float()\n",
        "\n",
        "class NoamOpt:\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        self._step = 0\n",
        "\n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        rate = self.learning_rate()\n",
        "        for param in self.optimizer.param_groups:\n",
        "            param['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def learning_rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "def make_model(source_vocab, target_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    c = copy.deepcopy\n",
        "    attention = MHA(h, d_model).to(DEVICE)\n",
        "    ff = FeedForward(d_model, d_ff, dropout).to(DEVICE)\n",
        "    position = EncodePositions(d_model, dropout).to(DEVICE)\n",
        "    model = Transformer(\n",
        "        Encoder(EncoderLayer(d_model, c(attention), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
        "        Decoder(DecoderLayer(d_model, c(attention), c(attention), c(ff), dropout).to(DEVICE), N).to(DEVICE),\n",
        "        nn.Sequential(CreateEmbeddings(d_model, source_vocab).to(DEVICE), c(position)),\n",
        "        nn.Sequential(CreateEmbeddings(d_model, target_vocab).to(DEVICE), c(position)),\n",
        "        Generator(d_model, target_vocab).to(DEVICE)\n",
        "    )\n",
        "    for param in model.parameters():\n",
        "        if param.dim() > 1:\n",
        "            nn.init.xavier_uniform_(param)\n",
        "    return model.to(DEVICE)\n",
        "\n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.source_embeddings[0].d_model, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    res = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for idx in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, Variable(res), Variable(subsequent_mask(res.size(1)).type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        res = torch.cat([res, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return res\n",
        "\n",
        "def run(data, model, loss_compute, epoch):\n",
        "    start = time.time()\n",
        "    total_tokens = 0.\n",
        "    total_loss = 0.\n",
        "    tokens = 0.\n",
        "    for idx, batch in enumerate(data):\n",
        "        out = model(batch.source, batch.outputs, batch.source_mask, batch.output_masks)\n",
        "        loss = loss_compute(out, batch.output_targets, batch.token_count)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.token_count\n",
        "        tokens += batch.token_count\n",
        "        if idx % 100 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch {:d}, Batch: {:d} Loss: {:.4f}\".format(epoch, idx - 1, loss / batch.token_count, (tokens.float() / elapsed / 1000.)))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "def train(data, model, criterion, optimizer):\n",
        "    best_loss = 1e5\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        run(data.train_data, model, LossComputation(model.generator, criterion, optimizer), epoch)\n",
        "        model.eval()\n",
        "\n",
        "        test_loss = run(data.test_data, model, LossComputation(model.generator, criterion, None), epoch)\n",
        "\n",
        "        if test_loss < best_loss:\n",
        "            best_loss = test_loss\n",
        "            torch.save(model.state_dict(), SAVE_CORPUS)\n",
        "\n",
        "        print(\"----------------------------\\n\")\n",
        "#%%\n",
        "# Training\n",
        "data = PreProcessing(TRAIN_CORPUS, TEST_CORPUS)\n",
        "src_vocab = len(data.dict_en)\n",
        "tgt_vocab = len(data.dict_zh)\n",
        "model = make_model(src_vocab, tgt_vocab, N=NUM_LAYERS, d_model=MODEL_DIMENSIONS, d_ff=FEED_FORWARD_DIMENSIONS, h=NUM_HEADS, dropout=DROPOUT)\n",
        "criterion = LabelSmoothing(size=tgt_vocab, padding_index=0, smoothing=0.1)\n",
        "model_opt = NoamOpt(model.source_embeddings[0].d_model, 1, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "train(data, model, criterion, model_opt)\n",
        "#%%\n",
        "# Evaluating\n",
        "def evaluate(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in np.random.randint(0, len(data.test_sequences_en), size=10):\n",
        "            english_sentence = \" \".join([data.index_dict_en[word] for word in data.test_sequences_en[i] if word != PAD])\n",
        "            zh_sentence = \" \".join([data.index_dict_zh[word] for word in data.test_sequences_zh[i] if word != PAD])\n",
        "            source = torch.from_numpy(np.array(data.test_sequences_en[i])).long().to(DEVICE).unsqueeze(0)\n",
        "            source_mask = (source != PAD).unsqueeze(-2)\n",
        "            out = greedy_decode(model, source, source_mask, max_len=MAX_LENGTH, start_symbol=data.dict_zh[\"BOS\"])\n",
        "            translation = []\n",
        "            for j in range(1, out.size(1)):\n",
        "                sym = data.index_dict_zh[out[0, j].item()]\n",
        "                if sym != 'EOS':\n",
        "                    translation.append(sym)\n",
        "                else:\n",
        "                    break\n",
        "            print(\"\\nOriginal English Sentence: \", english_sentence)\n",
        "            print(\"Target Chinese Sentence: \", zh_sentence)\n",
        "            print(\"Translated Chinese Sentence: \", \" \".join(translation))\n",
        "\n",
        "model.load_state_dict(torch.load(SAVE_CORPUS))\n",
        "evaluate(data, model)\n",
        "#%%\n",
        "# BLEU SCORE\n",
        "import random\n",
        "import sacrebleu\n",
        "\n",
        "def calculate_bleu_sacrebleu(data, model, device, sample_size):\n",
        "    model.eval()\n",
        "    sampled_data = random.sample(list(zip(data.test_sequences_en, data.test_sequences_zh)), sample_size)\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, ref in sampled_data:\n",
        "            ref_words = [data.index_dict_zh[word_id] for word_id in ref if word_id != data.dict_zh['PAD']]\n",
        "            references.append([' '.join(ref_words)])\n",
        "\n",
        "            src_tensor = torch.tensor([data.dict_en.get(word, data.dict_en['UNK']) for word in src], dtype=torch.long).to(device).unsqueeze(0)\n",
        "            src_mask = (src_tensor != data.dict_en['PAD']).unsqueeze(-2)\n",
        "\n",
        "            out = greedy_decode(model, src_tensor, src_mask, max_len=MAX_LENGTH, start_symbol=data.dict_zh[\"BOS\"])\n",
        "            translation = []\n",
        "            for token_id in out[0, :]:\n",
        "                word = data.index_dict_zh.get(token_id.item(), '[UNK]')\n",
        "                if word == 'EOS':\n",
        "                    break\n",
        "                translation.append(word)\n",
        "\n",
        "            hypotheses.append(' '.join(translation))\n",
        "\n",
        "    bleu_score = sacrebleu.corpus_bleu(hypotheses, references)\n",
        "    return bleu_score.score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(SAVE_CORPUS))\n",
        "bleu_start = time.time()\n",
        "bleu_score = calculate_bleu_sacrebleu(data, model, device, sample_size=500)\n",
        "print(f\"SacreBLEU Score on Development Set: {bleu_score:.2f}; Time: {time.time() - bleu_start:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "lw876diHiy3r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "nteract": {
      "version": "0.25.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "557px",
        "left": "447px",
        "right": "20px",
        "top": "255px",
        "width": "683px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}